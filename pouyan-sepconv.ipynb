{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61780e4e-d34c-446a-8140-b83516cf9a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "clear all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a72220e-6be1-43bd-bc16-a8c0a8733d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy import signal\n",
    "from scipy.fft import fft\n",
    "import random\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "845450e2-3c98-4186-a5e1-5fbf825b462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "faultI10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_1_10hz.mat')\n",
    "faultI30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_1_30hz.mat')\n",
    "faultII10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_2_10hz.mat')\n",
    "faultII30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_2_30hz.mat')\n",
    "faultIII10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_3_10hz.mat')\n",
    "faultIII30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_3_30hz.mat')\n",
    "faultIV10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_4_10hz.mat')\n",
    "faultIV30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_4_30hz.mat')\n",
    "faultV10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_5_10hz.mat')\n",
    "faultV30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_5_30hz.mat')\n",
    "# faultVI10=scipy.io.loadmat('C:/Users/PikPik/Desktop/Rotating Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_6_10hz.mat')\n",
    "# faultVI30=scipy.io.loadmat('C:/Users/PikPik/Desktop/Rotating Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_6_30hz.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5269e4f8-ae36-4662-9e37-c4aa4d47a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigII30=faultII30['Acquisition_un_2_30hz']\n",
    "sigII10=faultII10['Acquisition_un_2_10hz']\n",
    "del faultII30,faultII10\n",
    "sigI30=faultI30['Acquisition_un_1_30hz']\n",
    "sigI10=faultI10['Acquisition_un_1_10hz']\n",
    "del faultI30,faultI10\n",
    "sigIII10=faultIII10['Acquisition_un_3_10hz']\n",
    "sigIII30=faultIII30['Acquisition_un_3_30hz']\n",
    "del faultIII10,faultIII30\n",
    "sigIV10=faultIV10['Acquisition_un_4_10hz']\n",
    "sigIV30=faultIV30['Acquisition_un_4_30hz']\n",
    "del faultIV10,faultIV30\n",
    "sigV10=faultV10['Acquisition_un_5_10hz']\n",
    "sigV30=faultV30['Acquisition_un_5_30hz']\n",
    "del faultV10,faultV30\n",
    "# sigVI10=faultVI10['Acquisition_un_6_10hz']\n",
    "# sigVI30=faultVI30['Acquisition_un_6_30hz']\n",
    "# del faultVI10,faultVI30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d50f72e3-06c2-490f-865c-2c5005fa2307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sliding_window(data):\n",
    "    \"\"\"\n",
    "    Applies a sliding window to the given data with the specified window size\n",
    "    and stride length. Padding is added at the beginning and end of the data to\n",
    "    ensure all samples are included in windows.\n",
    "    \"\"\"\n",
    "    # num_padding = window_size - stride\n",
    "    # data_padded = np.pad(data, ((num_padding, num_padding), (0, 0)), 'constant')\n",
    "    # window_data = []\n",
    "    # for i in range(0, len(data_padded) - window_size + 1, stride):\n",
    "    #     window = data_padded[i:i+window_size, :]\n",
    "    #     window_data.append(window)\n",
    "    # window_data = np.array(window_data)\n",
    "    # window_data = window_data[:training_size, :, :]\n",
    "    # window_data = np.transpose(window_data, (0, 2, 1))\n",
    "    # return window_data\n",
    "    # Assuming that you have your data stored in a numpy array called \"data\"\n",
    "    fs = 1000 # Sampling frequency of the data\n",
    "    window_size = 256 # Number of samples in each window\n",
    "    overlap = 128 # Overlap between windows\n",
    "\n",
    "    # Define the window function to be used for the STFT\n",
    "    window = signal.hamming(window_size)\n",
    "\n",
    "    # Compute the STFT using the hamming window\n",
    "    f, t, Zxx = signal.stft(data, fs=fs, window=window, nperseg=window_size, noverlap=overlap)\n",
    "\n",
    "    \n",
    "    num_padding = window_size - overlap\n",
    "    data_padded = np.pad(data, ((num_padding, num_padding), (0, 0)), 'constant')\n",
    "    window_data = []\n",
    "    for i in range(0, len(data_padded) - window_size + 1, stride):\n",
    "        window = data_padded[i:i+window_size, :]\n",
    "        window_data.append(window)\n",
    "    window_data = np.array(window_data)\n",
    "    \n",
    "    window_data = np.transpose(window_data, (0, 2, 1))\n",
    "    \n",
    "    \n",
    "    plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=np.max(np.abs(Zxx)), shading='gouraud')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    return window_data\n",
    "    # Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bfde48d-37a8-4bda-b078-ac14f9aaead8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "window is longer than input signal",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sigtI10hz\u001b[39m=\u001b[39msliding_window(sigI10)[:,\u001b[39m1\u001b[39m:\u001b[39m13\u001b[39m,:]\n\u001b[1;32m      2\u001b[0m sigtI30hz\u001b[39m=\u001b[39msliding_window(sigI30)[:,\u001b[39m1\u001b[39m:\u001b[39m13\u001b[39m,:]\n\u001b[1;32m      3\u001b[0m sigtII10hz\u001b[39m=\u001b[39msliding_window(sigII10)[:,\u001b[39m1\u001b[39m:\u001b[39m13\u001b[39m,:]\n",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m, in \u001b[0;36msliding_window\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     23\u001b[0m window \u001b[39m=\u001b[39m signal\u001b[39m.\u001b[39mhamming(window_size)\n\u001b[1;32m     25\u001b[0m \u001b[39m# Compute the STFT using the hamming window\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m f, t, Zxx \u001b[39m=\u001b[39m signal\u001b[39m.\u001b[39;49mstft(data, fs\u001b[39m=\u001b[39;49mfs, window\u001b[39m=\u001b[39;49mwindow, nperseg\u001b[39m=\u001b[39;49mwindow_size, noverlap\u001b[39m=\u001b[39;49moverlap)\n\u001b[1;32m     29\u001b[0m num_padding \u001b[39m=\u001b[39m window_size \u001b[39m-\u001b[39m overlap\n\u001b[1;32m     30\u001b[0m data_padded \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpad(data, ((num_padding, num_padding), (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)), \u001b[39m'\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:1211\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(x, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, boundary, padded, axis, scaling)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[39melif\u001b[39;00m scaling \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mspectrum\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   1209\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m{\u001b[39;00mscaling\u001b[39m=}\u001b[39;00m\u001b[39m not in [\u001b[39m\u001b[39m'\u001b[39m\u001b[39mspectrum\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpsd\u001b[39m\u001b[39m'\u001b[39m\u001b[39m]!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1211\u001b[0m freqs, time, Zxx \u001b[39m=\u001b[39m _spectral_helper(x, x, fs, window, nperseg, noverlap,\n\u001b[1;32m   1212\u001b[0m                                     nfft, detrend, return_onesided,\n\u001b[1;32m   1213\u001b[0m                                     scaling\u001b[39m=\u001b[39;49mscaling, axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   1214\u001b[0m                                     mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mstft\u001b[39;49m\u001b[39m'\u001b[39;49m, boundary\u001b[39m=\u001b[39;49mboundary,\n\u001b[1;32m   1215\u001b[0m                                     padded\u001b[39m=\u001b[39;49mpadded)\n\u001b[1;32m   1217\u001b[0m \u001b[39mreturn\u001b[39;00m freqs, time, Zxx\n",
      "File \u001b[0;32m~/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:1795\u001b[0m, in \u001b[0;36m_spectral_helper\u001b[0;34m(x, y, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, mode, boundary, padded)\u001b[0m\n\u001b[1;32m   1792\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mnperseg must be a positive integer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1794\u001b[0m \u001b[39m# parse window; if array like, then set nperseg = win.shape\u001b[39;00m\n\u001b[0;32m-> 1795\u001b[0m win, nperseg \u001b[39m=\u001b[39m _triage_segments(window, nperseg, input_length\u001b[39m=\u001b[39;49mx\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m   1797\u001b[0m \u001b[39mif\u001b[39;00m nfft \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1798\u001b[0m     nfft \u001b[39m=\u001b[39m nperseg\n",
      "File \u001b[0;32m~/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:2024\u001b[0m, in \u001b[0;36m_triage_segments\u001b[0;34m(window, nperseg, input_length)\u001b[0m\n\u001b[1;32m   2022\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mwindow must be 1-D\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   2023\u001b[0m \u001b[39mif\u001b[39;00m input_length \u001b[39m<\u001b[39m win\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[0;32m-> 2024\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mwindow is longer than input signal\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   2025\u001b[0m \u001b[39mif\u001b[39;00m nperseg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2026\u001b[0m     nperseg \u001b[39m=\u001b[39m win\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: window is longer than input signal"
     ]
    }
   ],
   "source": [
    "sigtI10hz=sliding_window(sigI10)[:,1:13,:]\n",
    "sigtI30hz=sliding_window(sigI30)[:,1:13,:]\n",
    "sigtII10hz=sliding_window(sigII10)[:,1:13,:]\n",
    "sigtII30hz=sliding_window(sigII30)[:,1:13,:]\n",
    "del sigI10,sigI30,sigII10,sigII30\n",
    "sigtIII30hz=sliding_window(sigIII30)[:,1:13,:]\n",
    "sigtIII10hz=sliding_window(sigIII10)[:,1:13,:]\n",
    "sigtIV30hz=sliding_window(sigIV30)[:,1:13,:]\n",
    "sigtIV10hz=sliding_window(sigIV10)[:,1:13,:]\n",
    "del sigIII30,sigIII10,sigIV30,sigIV10\n",
    "sigtV30hz=sliding_window(sigV30)[:,1:13,:]\n",
    "sigtV10hz=sliding_window(sigV10)[:,1:13,:]\n",
    "# sigtVI30hz=sliding_window(sigVI30)\n",
    "# sigtVI10hz=sliding_window(sigVI10)\n",
    "# del sigV30,sigV10,sigVI30,sigVI10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12eddff8-d75a-4f2d-b477-1ac635306b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16250293-9d83-4628-a56a-22bc284fa5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1506, 12, 500)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sigtI10hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "714768e2-f5bc-49be-9c25-ffddc9c12501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_last_dim(data, epsilon=0.01):\n",
    "    # compute mean and standard deviation along the last dimension of the data\n",
    "    last_dim_mean = np.mean(data, axis=-1, keepdims=True)\n",
    "    last_dim_std = np.std(data, axis=-1, keepdims=True)\n",
    "\n",
    "    # normalize the last dimension using the mean and standard deviation\n",
    "    normalized_data = (data- last_dim_mean) /last_dim_std+epsilon\n",
    "\n",
    "#     # stack the normalized values with the original values along the last dimension\n",
    "    # normalized_data = np.concatenate([data[..., :-1], normalized_data[..., np.newaxis]], axis=-1)\n",
    "\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a04478d-e060-447b-8dc0-baad5aaa89fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1n=normalize_last_dim(sigtI10hz)\n",
    "del sigtI10hz\n",
    "a2n=normalize_last_dim(sigtI30hz)\n",
    "del sigtI30hz\n",
    "a3n=normalize_last_dim(sigtII10hz)\n",
    "del sigtII10hz\n",
    "a4n=normalize_last_dim(sigtII30hz)\n",
    "del sigtII30hz\n",
    "a5n=normalize_last_dim(sigtIII10hz)\n",
    "del sigtIII10hz\n",
    "a6n=normalize_last_dim(sigtIII30hz)\n",
    "del sigtIII30hz\n",
    "a7n=normalize_last_dim(sigtIV10hz)\n",
    "del sigtIV10hz\n",
    "a8n=normalize_last_dim(sigtIV30hz)\n",
    "del sigtIV30hz\n",
    "a9n=normalize_last_dim(sigtV10hz)\n",
    "del sigtV10hz\n",
    "a10n=normalize_last_dim(sigtV30hz)\n",
    "del sigtV30hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85ca8a94-a698-4317-b31a-08a43c07cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalable1=np.concatenate([a1n,a2n],axis=-1)\n",
    "del a1n,a2n\n",
    "datalable2=np.concatenate([a3n,a4n],axis=-1)\n",
    "del a3n,a4n\n",
    "datalable3=np.concatenate([a5n,a6n],axis=-1)\n",
    "del a5n,a6n\n",
    "datalable4=np.concatenate([a7n,a8n],axis=-1)\n",
    "del a7n,a8n\n",
    "datalabele5=np.concatenate([a9n,a10n],axis=-1)\n",
    "del a9n,a10n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1e0558a-8759-4699-ae4e-4d6bc0de362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=np.concatenate((np.ones(np.shape(datalable1)[0]),np.zeros(np.shape(datalable1)[0]),np.ones(np.shape(datalable1)[0])*4,np.ones(np.shape(datalable1)[0])*2,np.ones(np.shape(datalable1)[0])*3),axis=0)\n",
    "dataall=np.concatenate([datalable1,datalable2,datalable3,datalable4,datalabele5],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91ce792f-099c-4c60-9734-0d6555a1f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(dataall.shape[0])\n",
    "\n",
    "# Use the shuffled indices to shuffle both data and labels\n",
    "shuffled_data = dataall[indices]\n",
    "shuffled_labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb31a0-00a0-4e0b-8f82-7a8026d58901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fb0c61b-a5e6-496b-ba7c-b6447bc7b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data_equally(X, y, test_size=0.4, random_state=None):\n",
    "    # Get unique labels in y and their indices\n",
    "    labels, indices = np.unique(y, return_index=True)\n",
    "    n_labels = len(labels)\n",
    "    \n",
    "    # Split each label's indices into test and train\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    for i in range(n_labels):\n",
    "        label_indices = np.where(y == labels[i])[0]\n",
    "        label_train_indices, label_test_indices = train_test_split(label_indices, test_size=test_size, \n",
    "                                                                    random_state=random_state)\n",
    "        train_indices += list(label_train_indices)\n",
    "        test_indices += list(label_test_indices)\n",
    "    \n",
    "    # Shuffle train and test indices\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "    # Get corresponding data and labels\n",
    "    X_train, y_train = X[train_indices], y[train_indices]\n",
    "    X_test, y_test = X[test_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea210173-b18b-4786-9551-6db9823f80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data_equally(shuffled_data,shuffled_labels,test_size=0.3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "950db405-594b-4aac-a6e8-237387fc322a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5270, 12, 1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d863537-4398-42df-8835-5d344012afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def create_signal_classifier(input_channels, num_classes, dilation_rates=[1, 2, 4]):\n",
    "    # Define the convolutional layers\n",
    "    conv_layers = []\n",
    "    in_channels = input_channels\n",
    "    for dilation_rate in dilation_rates:\n",
    "        out_channels = 2 * in_channels\n",
    "        conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=(1, 1),\n",
    "                               padding=dilation_rate, dilation=dilation_rate)\n",
    "        conv_layers.append(conv_layer)\n",
    "        conv_layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "\n",
    "    # Define the fully connected layers\n",
    "    fc_layers = [\n",
    "        nn.Linear(in_channels * 4 * 4, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, num_classes)\n",
    "    ]\n",
    "\n",
    "    # Combine the convolutional and fully connected layers into a neural network\n",
    "    net = nn.Sequential(*conv_layers, nn.MaxPool2d(kernel_size=(2, 2)), nn.Flatten(), *fc_layers)\n",
    "\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e480a24-6492-4d19-a9af-9d6e856fcae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    # Convolutional layer with 32 filters, each 3x3 in size\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(12, np.shape(dataall)[2])),\n",
    "    # Max pooling layer with pool size of 2\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    # Flatten the output of the convolutional layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # Dense layer with 64 units\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    # Output layer with 5 units (one for each label)\n",
    "    tf.keras.layers.Dense(units=5, activation='softmax')\n",
    "])\n",
    "\n",
    "# # Compile the model\n",
    "# optimizer = keras.optimizers.Adam(lr=0.0005)\n",
    "# model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaf4bf3e-408e-4cda-94b2-824d1a052533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Convolutional layer with 32 filters, each 3x3 in size\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(12, np.shape(dataall)[2])),\n",
    "    # Max pooling layer with pool size of 2\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    # Additional Convolutional layer with 64 filters, each 3x3 in size\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    # Additional Max pooling layer with pool size of 2\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    # Flatten the output of the convolutional layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # Additional Dense layer with 128 units\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    # Additional Dropout layer with rate of 0.2\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    # Dense layer with 64 units\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    # Output layer with 5 units (one for each label)\n",
    "    tf.keras.layers.Dense(units=5, activation='softmax')\n",
    "])\n",
    "# optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "# model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b502198-862d-479f-8f4f-d7289a1593f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# # Define callbacks\n",
    "# early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "# checkpoint_callback = ModelCheckpoint(filepath='model_weights.h5', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# # Train the model with callbacks\n",
    "# model.fit(X_train, y_train, epochs=20, batch_size=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab76447-b79d-492d-b515-a5fd84e07c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PikPik\\anaconda\\anaconda latest\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/100\n",
      "844/844 [==============================] - 11s 9ms/step - loss: 1.4290 - accuracy: 0.3366 - val_loss: 0.8270 - val_accuracy: 0.6157 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/100\n",
      "844/844 [==============================] - 4s 5ms/step - loss: 0.6870 - accuracy: 0.6914 - val_loss: 0.6250 - val_accuracy: 0.7192 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/100\n",
      "844/844 [==============================] - 4s 5ms/step - loss: 0.3869 - accuracy: 0.8465 - val_loss: 0.4677 - val_accuracy: 0.7951 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/100\n",
      "844/844 [==============================] - 5s 6ms/step - loss: 0.2198 - accuracy: 0.9224 - val_loss: 0.5125 - val_accuracy: 0.8046 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/100\n",
      "844/844 [==============================] - 5s 5ms/step - loss: 0.1483 - accuracy: 0.9469 - val_loss: 0.6357 - val_accuracy: 0.7998 - lr: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/100\n",
      "844/844 [==============================] - 4s 5ms/step - loss: 0.0831 - accuracy: 0.9734 - val_loss: 0.6065 - val_accuracy: 0.8302 - lr: 0.0010\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/100\n",
      "844/844 [==============================] - 5s 6ms/step - loss: 0.0996 - accuracy: 0.9661 - val_loss: 0.5968 - val_accuracy: 0.8349 - lr: 0.0010\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.00017999999999999998.\n",
      "Epoch 8/100\n",
      "844/844 [==============================] - 5s 6ms/step - loss: 0.0343 - accuracy: 0.9903 - val_loss: 0.5210 - val_accuracy: 0.8558 - lr: 1.8000e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.00017999999999999998.\n",
      "Epoch 9/100\n",
      "844/844 [==============================] - 5s 6ms/step - loss: 0.0083 - accuracy: 0.9979 - val_loss: 0.5230 - val_accuracy: 0.8643 - lr: 1.8000e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.00017999999999999998.\n",
      "Epoch 10/100\n",
      "844/844 [==============================] - 6s 8ms/step - loss: 0.0050 - accuracy: 0.9995 - val_loss: 0.5369 - val_accuracy: 0.8624 - lr: 1.8000e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.00017999999999999998.\n",
      "Epoch 11/100\n",
      "844/844 [==============================] - 5s 6ms/step - loss: 0.0028 - accuracy: 0.9998 - val_loss: 0.6062 - val_accuracy: 0.8624 - lr: 1.8000e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00017999999999999998.\n",
      "Epoch 12/100\n",
      "844/844 [==============================] - 5s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5918 - val_accuracy: 0.8624 - lr: 1.8000e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.00017999999999999998.\n",
      "Epoch 13/100\n",
      "280/844 [========>.....................] - ETA: 2s - loss: 7.5701e-04 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Define a function to decay the learning rate\n",
    "def lr_decay(epoch):\n",
    "    initial_lr = 0.001\n",
    "    decay_rate = 0.18\n",
    "    decay_step = 7\n",
    "    lr = initial_lr * (decay_rate ** (epoch // decay_step))\n",
    "    return lr\n",
    "\n",
    "# Create an instance of the LearningRateScheduler callback\n",
    "lr_scheduler = LearningRateScheduler(lr_decay, verbose=1)\n",
    "\n",
    "# Create an optimizer and add the learning rate scheduler callback\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=100,  batch_size=5,validation_split=0.2, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c3daf-a92d-4a76-b01d-c1b7c7bb0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((np.argmax(model(X_test),axis=1)==y_test)*1)/2260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "768bc8bb-825a-4725-ab38-bd593b2d7a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2260, 12, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "49777eea-421f-4581-9817-05d9feef0613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 3., 3., 3.])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc74e6eb-a1e2-4d22-aecb-147e2c8fea1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
