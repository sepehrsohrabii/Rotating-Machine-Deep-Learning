{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61780e4e-d34c-446a-8140-b83516cf9a55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61780e4e-d34c-446a-8140-b83516cf9a55",
    "outputId": "ecb7990e-b319-431c-801d-6dbe72d65125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "clear all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a72220e-6be1-43bd-bc16-a8c0a8733d4a",
   "metadata": {
    "id": "2a72220e-6be1-43bd-bc16-a8c0a8733d4a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.fft import fft\n",
    "import random\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "zPtG3_qt8YGd",
   "metadata": {
    "id": "zPtG3_qt8YGd"
   },
   "outputs": [],
   "source": [
    "faultI10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_1_10hz.mat')\n",
    "faultI30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_1_30hz.mat')\n",
    "faultII10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_2_10hz.mat')\n",
    "faultII30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_2_30hz.mat')\n",
    "faultIII10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_3_10hz.mat')\n",
    "faultIII30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_3_30hz.mat')\n",
    "faultIV10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_4_10hz.mat')\n",
    "faultIV30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_4_30hz.mat')\n",
    "faultV10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_5_10hz.mat')\n",
    "faultV30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_5_30hz.mat')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0m2Uist6LyMj",
   "metadata": {
    "id": "0m2Uist6LyMj"
   },
   "outputs": [],
   "source": [
    "sigII30=faultII30['Acquisition_un_2_30hz']\n",
    "sigII10=faultII10['Acquisition_un_2_10hz']\n",
    "del faultII30,faultII10\n",
    "sigI30=faultI30['Acquisition_un_1_30hz']\n",
    "sigI10=faultI10['Acquisition_un_1_10hz']\n",
    "del faultI30,faultI10\n",
    "sigIII10=faultIII10['Acquisition_un_3_10hz']\n",
    "sigIII30=faultIII30['Acquisition_un_3_30hz']\n",
    "del faultIII10,faultIII30\n",
    "sigIV10=faultIV10['Acquisition_un_4_10hz']\n",
    "sigIV30=faultIV30['Acquisition_un_4_30hz']\n",
    "del faultIV10,faultIV30\n",
    "sigV10=faultV10['Acquisition_un_5_10hz']\n",
    "sigV30=faultV30['Acquisition_un_5_30hz']\n",
    "del faultV10,faultV30\n",
    "# sigVI10=faultVI10['Acquisition_un_6_10hz']\n",
    "# sigVI30=faultVI30['Acquisition_un_6_30hz']\n",
    "# del faultVI10,faultVI30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8BhBzYin7SFO",
   "metadata": {
    "id": "8BhBzYin7SFO"
   },
   "outputs": [],
   "source": [
    "\n",
    "def sliding_window(data, window_size, stride):\n",
    "    \"\"\"\n",
    "    Applies a sliding window to the given data with the specified window size\n",
    "    and stride length. Padding is added at the beginning and end of the data to\n",
    "    ensure all samples are included in windows.\n",
    "    \"\"\"\n",
    "    num_padding = window_size - stride\n",
    "    data_padded = np.pad(data, ((num_padding, num_padding), (0, 0)), 'constant')\n",
    "    window_data = []\n",
    "    for i in range(0, len(data_padded) - window_size + 1, stride):\n",
    "        window = data_padded[i:i+window_size, :]\n",
    "        window_data.append(window)\n",
    "    window_data = np.array(window_data)\n",
    "    # window_data = window_data[:training_size, :, :]\n",
    "    window_data = np.transpose(window_data, (0, 2, 1))\n",
    "    return window_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cWxOnPFS7URd",
   "metadata": {
    "id": "cWxOnPFS7URd"
   },
   "outputs": [],
   "source": [
    "windowsize=1500\n",
    "stride=1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "zAVETMOk7XKF",
   "metadata": {
    "id": "zAVETMOk7XKF"
   },
   "outputs": [],
   "source": [
    "sigtI10hz=sliding_window(sigI10,windowsize,stride)[:,1:13,:]\n",
    "sigtI30hz=sliding_window(sigI30,windowsize,stride)[:,1:13,:]\n",
    "sigtII10hz=sliding_window(sigII10,windowsize,stride)[:,1:13,:]\n",
    "sigtII30hz=sliding_window(sigII30,windowsize,stride)[:,1:13,:]\n",
    "del sigI10,sigI30,sigII10,sigII30\n",
    "sigtIII30hz=sliding_window(sigIII30,windowsize,stride)[:,1:13,:]\n",
    "sigtIII10hz=sliding_window(sigIII10,windowsize,stride)[:,1:13,:]\n",
    "sigtIV30hz=sliding_window(sigIV30,windowsize,stride)[:,1:13,:]\n",
    "sigtIV10hz=sliding_window(sigIV10,windowsize,stride)[:,1:13,:]\n",
    "del sigIII30,sigIII10,sigIV30,sigIV10\n",
    "sigtV30hz=sliding_window(sigV30,windowsize,stride)[:,1:13,:]\n",
    "sigtV10hz=sliding_window(sigV10,windowsize,stride)[:,1:13,:]\n",
    "# sigtVI30hz=sliding_window(sigVI30,windowsize,stride)\n",
    "# sigtVI10hz=sliding_window(sigVI10,windowsize,stride)\n",
    "# del sigV30,sigV10,sigVI30,sigVI10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eztt9Ct5MMrb",
   "metadata": {
    "id": "eztt9Ct5MMrb"
   },
   "outputs": [],
   "source": [
    "datalable1=np.concatenate([sigtI10hz,sigtI30hz],axis=-1)\n",
    "\n",
    "datalable2=np.concatenate([sigtII10hz,sigtII30hz],axis=-1)\n",
    "\n",
    "datalable3=np.concatenate([sigtIII10hz,sigtIII30hz],axis=-1)\n",
    "\n",
    "datalable4=np.concatenate([sigtIV10hz,sigtIV30hz],axis=-1)\n",
    "\n",
    "datalabele5=np.concatenate([sigtV10hz,sigtV30hz],axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "XfUB8VSqMT97",
   "metadata": {
    "id": "XfUB8VSqMT97"
   },
   "outputs": [],
   "source": [
    "#labels=np.concatenate((np.ones(np.shape(datalable1)[0]),np.zeros(np.shape(datalable1)[0]),np.ones(np.shape(datalable1)[0])*4,np.ones(np.shape(datalable1)[0])*2,np.ones(np.shape(datalable1)[0])*3),axis=0)\n",
    "dataall=np.concatenate([datalable1,datalable2,datalable3,datalable4,datalabele5],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "l0TCUypy5LqU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0TCUypy5LqU",
    "outputId": "df93d2dd-13e8-4fff-c8d9-d5ea4b40e9ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2150, 12, 3000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(dataall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "JiqTSASTWFs2",
   "metadata": {
    "id": "JiqTSASTWFs2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import stft\n",
    "\n",
    "# generate example signal with shape (7530, 12, 1000)\n",
    "data = dataall\n",
    "\n",
    "# set STFT parameters\n",
    "window = 'hann'\n",
    "nperseg = 256\n",
    "noverlap = 128\n",
    "fs = 10000\n",
    "\n",
    "# compute STFT spectrogram for each channel\n",
    "spectrograms = []\n",
    "for i in range(data.shape[1]):\n",
    "    _, _, spec = stft(data[:, i, :], fs=fs, window=window, nperseg=nperseg, noverlap=noverlap)\n",
    "    spectrograms.append(np.abs(spec))\n",
    "\n",
    "# stack spectrograms along the channel axis to get shape (7530, 12, num_freq_bins, num_time_bins)\n",
    "spectrograms = np.stack(spectrograms, axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "re1z7vnHYiNX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "re1z7vnHYiNX",
    "outputId": "0ecb1543-eefe-46d3-a5ae-cb8f39b30bcd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2150, 12, 129, 25)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "EViGZC2O8jzI",
   "metadata": {
    "id": "EViGZC2O8jzI"
   },
   "outputs": [],
   "source": [
    "mean = np.mean(spectrograms, axis=(0, 1, 2, 3))\n",
    "std = np.std(spectrograms, axis=(0, 1, 2, 3))\n",
    "normalized_data = (spectrograms - mean) / std\n",
    "data=normalized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "YavNwJdd8yHf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YavNwJdd8yHf",
    "outputId": "fe827e8c-8806-4dcd-81eb-867b1588655d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2150, 12, 129, 25)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "XEZZREKkVVu3",
   "metadata": {
    "id": "XEZZREKkVVu3"
   },
   "outputs": [],
   "source": [
    "labels=np.concatenate((np.ones(np.shape(datalable1)[0]),np.zeros(np.shape(datalable1)[0]),np.ones(np.shape(datalable1)[0])*4,np.ones(np.shape(datalable1)[0])*2,np.ones(np.shape(datalable1)[0])*3),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "HIg0JQ-4hC68",
   "metadata": {
    "id": "HIg0JQ-4hC68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 3., 3., 3.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4tuT02XWDPjo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tuT02XWDPjo",
    "outputId": "5002f132-ebe2-4ba1-fb56-aa4af79ddcb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2150"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "-0DLnNclARZA",
   "metadata": {
    "id": "-0DLnNclARZA"
   },
   "outputs": [],
   "source": [
    "indices = np.random.permutation(data.shape[0])\n",
    "\n",
    "# Use the shuffled indices to shuffle both data and labels\n",
    "shuffled_data = data[indices]\n",
    "shuffled_labels = labels[indices]\n",
    "data= shuffled_data\n",
    "labels=shuffled_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7912fb42-e76b-4f68-b22a-84dd09ad4329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 4., 0., ..., 4., 3., 0.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1xjKJqwXA2tY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xjKJqwXA2tY",
    "outputId": "d32fa1fb-a1d3-499a-dbe2-62896ec4dbde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1720, 12, 129, 25)\n",
      "y_train shape: (1720,)\n",
      "x_test shape: (430, 12, 129, 25)\n",
      "y_test shape: (430,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume stft_data is your STFT results stored in a variable\n",
    "# with shape (7530, 12, 129, 9)\n",
    "\n",
    "# Generate labels for your data (assuming you have labels available)\n",
    "\n",
    "\n",
    "# Split the data and labels into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")  # should be (6024, 12, 129, 9)\n",
    "print(f\"y_train shape: {y_train.shape}\")  # should be (6024,)\n",
    "print(f\"x_test shape: {x_test.shape}\")    # should be (1506, 12, 129, 9)\n",
    "print(f\"y_test shape: {y_test.shape}\")    # should be (1506,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7165cd6a-6994-47bf-81f6-3cb776dd14bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train1 = to_categorical(y_train)\n",
    "\n",
    "y_test1 = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "07eece98-e4fd-455f-8f7c-75a611b3fbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "7/7 [==============================] - 4s 499ms/step - loss: 1.8008 - accuracy: 0.2119 - val_loss: 1.6173 - val_accuracy: 0.1977\n",
      "Epoch 2/40\n",
      "7/7 [==============================] - 3s 434ms/step - loss: 1.6257 - accuracy: 0.2074 - val_loss: 1.6119 - val_accuracy: 0.1919\n",
      "Epoch 3/40\n",
      "7/7 [==============================] - 3s 444ms/step - loss: 1.6227 - accuracy: 0.1938 - val_loss: 1.6102 - val_accuracy: 0.1919\n",
      "Epoch 4/40\n",
      "7/7 [==============================] - 4s 518ms/step - loss: 1.6134 - accuracy: 0.2048 - val_loss: 1.6087 - val_accuracy: 0.1919\n",
      "Epoch 5/40\n",
      "7/7 [==============================] - 3s 433ms/step - loss: 1.6114 - accuracy: 0.2106 - val_loss: 1.6075 - val_accuracy: 0.2093\n",
      "Epoch 6/40\n",
      "7/7 [==============================] - 3s 431ms/step - loss: 1.6118 - accuracy: 0.2145 - val_loss: 1.6052 - val_accuracy: 0.1977\n",
      "Epoch 7/40\n",
      "7/7 [==============================] - 3s 432ms/step - loss: 1.6074 - accuracy: 0.2106 - val_loss: 1.6031 - val_accuracy: 0.2151\n",
      "Epoch 8/40\n",
      "7/7 [==============================] - 3s 409ms/step - loss: 1.6119 - accuracy: 0.2255 - val_loss: 1.5991 - val_accuracy: 0.2151\n",
      "Epoch 9/40\n",
      "7/7 [==============================] - 3s 429ms/step - loss: 1.6008 - accuracy: 0.2287 - val_loss: 1.5898 - val_accuracy: 0.5407\n",
      "Epoch 10/40\n",
      "7/7 [==============================] - 3s 427ms/step - loss: 1.5857 - accuracy: 0.2997 - val_loss: 1.5771 - val_accuracy: 0.2209\n",
      "Epoch 11/40\n",
      "7/7 [==============================] - 3s 477ms/step - loss: 1.5649 - accuracy: 0.2959 - val_loss: 1.5342 - val_accuracy: 0.3023\n",
      "Epoch 12/40\n",
      "7/7 [==============================] - 3s 461ms/step - loss: 1.5244 - accuracy: 0.3185 - val_loss: 1.4475 - val_accuracy: 0.5465\n",
      "Epoch 13/40\n",
      "7/7 [==============================] - 3s 421ms/step - loss: 1.4272 - accuracy: 0.3921 - val_loss: 1.4191 - val_accuracy: 0.3488\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_shape = (12, 129, 25)\n",
    "num_classes = 5\n",
    "# Define input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# First scale\n",
    "conv1 = Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(inputs)\n",
    "pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "\n",
    "# Second scale\n",
    "conv2 = Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "\n",
    "# Third scale\n",
    "conv3 = Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(pool2)\n",
    "pool3 = MaxPooling2D(pool_size=(2,2))(conv3)\n",
    "\n",
    "# Flatten and concatenate scales\n",
    "flatten = Flatten()(pool3)\n",
    "dense1 = Dense(128, activation='relu')(flatten)\n",
    "dropout1 = Dropout(0.5)(dense1)\n",
    "\n",
    "# Add final output layer\n",
    "outputs = Dense(num_classes, activation='softmax')(dropout1)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train1, validation_split=0.1, epochs=40, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2GhzbQeYRCtH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GhzbQeYRCtH",
    "outputId": "f1ad55f9-26d1-4c73-8ae4-1140ab6be71b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate the model on the test set with multi-class labels\n",
    "test_loss_multi, test_acc_multi = model.evaluate(x_test, y_test1, verbose=2)\n",
    "print('Test accuracy multi:', test_acc_multi)\n",
    "\n",
    "# Convert y_test1 to a multi-class format\n",
    "y_pred_multi = model.predict(x_test)\n",
    "y_test_pred_multi = np.argmax(y_pred_multi, axis=1)\n",
    "\n",
    "# Compute the confusion matrix for multi-class\n",
    "print('Multi-class:', confusion_matrix(np.argmax(y_test1, axis=1), y_test_pred_multi))\n",
    "\n",
    "# Evaluate the model on the test set with normal labels\n",
    "test_loss_normal, test_acc_normal = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test accuracy normal:', test_acc_normal)\n",
    "\n",
    "# Convert y_test to a multi-class format\n",
    "y_pred_normal = model.predict(x_test)\n",
    "y_test_pred_normal = np.argmax(y_pred_normal, axis=1)\n",
    "\n",
    "# Compute the confusion matrix for normal labels\n",
    "print('Normal labels:', confusion_matrix(y_test, y_test_pred_normal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca2c8b-2983-4a07-a0b7-26c7b4a1fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RsMdULceDvgB",
   "metadata": {
    "id": "RsMdULceDvgB"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9u9kUdTNDxdx",
   "metadata": {
    "id": "9u9kUdTNDxdx"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "# y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h2md8xShDzk5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "h2md8xShDzk5",
    "outputId": "83dd0a60-f3f2-4895-83d7-e780b70d6c36"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ofQF75O-UO_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "3ofQF75O-UO_",
    "outputId": "0717ac9a-0776-4704-d071-b9332e63c992"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LbjShG_vBYVV",
   "metadata": {
    "id": "LbjShG_vBYVV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UX_LG66wBlzS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "UX_LG66wBlzS",
    "outputId": "99487633-f28f-465a-e310-572138583c80"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S74Lcu0EAydh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "S74Lcu0EAydh",
    "outputId": "b44dcdab-c2c2-4f4e-8748-4d7df0128346"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume data is your STFT results stored in a variable\n",
    "# with shape (7530, 12, 129, 9)\n",
    "\n",
    "# Reshape the data to have a 1D input shape\n",
    "data_1d = data.reshape(data.shape[0], -1)\n",
    "\n",
    "# Generate labels for your data\n",
    "labels = ...\n",
    "\n",
    "# Split the data and labels into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_1d, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z36hRYUQAqUI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z36hRYUQAqUI",
    "outputId": "4041a4ef-df20-4d1f-f68c-8ccbb95637ac"
   },
   "outputs": [],
   "source": [
    "np.shape(data_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CzHcnwho-hUn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CzHcnwho-hUn",
    "outputId": "acca88fe-8c70-4a90-fa44-1d030628b217"
   },
   "outputs": [],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iQEXX31l_SB_",
   "metadata": {
    "id": "iQEXX31l_SB_"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FyobYbsd-43P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "FyobYbsd-43P",
    "outputId": "01a07994-14c1-4932-d13d-50e59afd8ce8"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 3, activation='relu', input_shape=(12, 1161)))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(32, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yUl_8HZU_RNG",
   "metadata": {
    "id": "yUl_8HZU_RNG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mYDSWWiy-MVQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mYDSWWiy-MVQ",
    "outputId": "b99b849d-51ab-48a5-bbcc-23bf5b5458c1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have 3 classes (0, 1, 2, 3, 4)\n",
    "num_classes = 5\n",
    "\n",
    "# Generate random labels for your STFT data\n",
    "labelss = np.random.randint(0, num_classes, size=(7530,))\n",
    "labelss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Eb6pOHDr-Kn9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eb6pOHDr-Kn9",
    "outputId": "b99b849d-51ab-48a5-bbcc-23bf5b5458c1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have 3 classes (0, 1, 2, 3, 4)\n",
    "num_classes = 5\n",
    "\n",
    "# Generate random labels for your STFT data\n",
    "labelss = np.random.randint(0, num_classes, size=(7530,))\n",
    "labelss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mrvkc7vO-Iu_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mrvkc7vO-Iu_",
    "outputId": "b99b849d-51ab-48a5-bbcc-23bf5b5458c1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have 3 classes (0, 1, 2, 3, 4)\n",
    "num_classes = 5\n",
    "\n",
    "# Generate random labels for your STFT data\n",
    "labelss = np.random.randint(0, num_classes, size=(7530,))\n",
    "labelss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UdPRz5jrZePv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 944
    },
    "id": "UdPRz5jrZePv",
    "outputId": "93166f4a-2b0c-4569-8e32-a5eab1a6c5ad"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select a trial to visualize\n",
    "trial_num = 10\n",
    "\n",
    "# Get the STFT spectrograms for all channels of the selected trial\n",
    "trial_stft = spectrograms[trial_num]\n",
    "\n",
    "# Set up the plot grid\n",
    "num_channels = trial_stft.shape[0]\n",
    "num_cols = 4\n",
    "num_rows = (num_channels + num_cols - 1) // num_cols\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(16, num_rows*4))\n",
    "\n",
    "# Plot each spectrogram\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    if i < num_channels:\n",
    "        spec = trial_stft[i]\n",
    "        img = ax.pcolormesh(spec, cmap='jet')\n",
    "        ax.set_title(f'Channel {i+1}')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        fig.colorbar(img, ax=ax)\n",
    "\n",
    "# Set overall title and spacing\n",
    "fig.suptitle(f'STFT Spectrograms for Trial {trial_num+1}')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lYI69D2Ijx7X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYI69D2Ijx7X",
    "outputId": "b99b849d-51ab-48a5-bbcc-23bf5b5458c1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have 3 classes (0, 1, 2, 3, 4)\n",
    "num_classes = 5\n",
    "\n",
    "# Generate random labels for your STFT data\n",
    "labelss = np.random.randint(0, num_classes, size=(7530,))\n",
    "labelss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ar7zz8PCkVML",
   "metadata": {
    "id": "ar7zz8PCkVML"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your STFT data\n",
    "X = spectrograms\n",
    "\n",
    "# Define your labels\n",
    "y = labelss\n",
    "\n",
    "# Split your data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4tSqkMLxsq0S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tSqkMLxsq0S",
    "outputId": "4e150f1a-dd6d-4c32-adf3-1bcafa43cbe1"
   },
   "outputs": [],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tIieCUwdYgBZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "tIieCUwdYgBZ",
    "outputId": "d9110ad3-4d85-4381-cdae-561f7dcecb32"
   },
   "outputs": [],
   "source": [
    "#new\n",
    "indices = np.random.permutation(dataall.shape[0])\n",
    "\n",
    "# Use the shuffled indices to shuffle both data and labels\n",
    "shuffled_data = dataall[indices]\n",
    "shuffled_labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VUly0im2mFB1",
   "metadata": {
    "id": "VUly0im2mFB1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "spec_data= spectrograms[:,:] \n",
    "# assume your spectrogram data is stored in variable called 'spec_data'\n",
    "num_samples = spec_data.shape[0]  # number of samples\n",
    "num_channels = spec_data.shape[1]  # number of channels\n",
    "num_freq_bins = spec_data.shape[2]  # number of frequency bins\n",
    "num_time_windows = spec_data.shape[3]  # number of time windows\n",
    "\n",
    "# create labels for your data (replace with your own labels)\n",
    "labels = np.random.randint(low=0, high=5, size=num_samples)\n",
    "\n",
    "# split data into training, validation, and testing sets (60-20-20 split)\n",
    "train_split = 0.6\n",
    "val_split = 0.2\n",
    "\n",
    "# calculate number of samples for each set\n",
    "num_train_samples = int(train_split * num_samples)\n",
    "num_val_samples = int(val_split * num_samples)\n",
    "num_test_samples = num_samples - num_train_samples - num_val_samples\n",
    "\n",
    "# shuffle indices of samples\n",
    "shuffled_indices = np.random.permutation(num_samples)\n",
    "\n",
    "# split indices into training, validation, and testing sets\n",
    "train_indices = shuffled_indices[:num_train_samples]\n",
    "val_indices = shuffled_indices[num_train_samples:num_train_samples+num_val_samples]\n",
    "test_indices = shuffled_indices[num_train_samples+num_val_samples:]\n",
    "\n",
    "# create empty arrays to store the split data and labels\n",
    "train_data = np.zeros((num_train_samples, num_channels, num_freq_bins, num_time_windows))\n",
    "val_data = np.zeros((num_val_samples, num_channels, num_freq_bins, num_time_windows))\n",
    "test_data = np.zeros((num_test_samples, num_channels, num_freq_bins, num_time_windows))\n",
    "train_labels = np.zeros(num_train_samples)\n",
    "val_labels = np.zeros(num_val_samples)\n",
    "test_labels = np.zeros(num_test_samples)\n",
    "\n",
    "# copy data and labels into the split arrays\n",
    "train_data[:] = spec_data[train_indices]\n",
    "val_data[:] = spec_data[val_indices]\n",
    "test_data[:] = spec_data[test_indices]\n",
    "train_labels[:] = labels[train_indices]\n",
    "val_labels[:] = labels[val_indices]\n",
    "test_labels[:] = labels[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hqk9yChmwkgx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hqk9yChmwkgx",
    "outputId": "edb1385f-4e1f-4dfa-8fec-cd33a4456efb"
   },
   "outputs": [],
   "source": [
    "np.shape(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pMgbS83Dv7DB",
   "metadata": {
    "id": "pMgbS83Dv7DB"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "val_labels_encoded = label_encoder.transform(val_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Convert the labels to categorical format\n",
    "train_labels_categorical = to_categorical(train_labels_encoded)\n",
    "val_labels_categorical = to_categorical(val_labels_encoded)\n",
    "test_labels_categorical = to_categorical(test_labels_encoded)\n",
    "\n",
    "# Reshape the data for 1D CNN\n",
    "train_data_1d = train_data.reshape(train_data.shape[0], train_data.shape[1]*train_data.shape[2])\n",
    "val_data_1d = val_data.reshape(val_data.shape[0], val_data.shape[1]*val_data.shape[2])\n",
    "test_data_1d = test_data.reshape(test_data.shape[0], test_data.shape[1]*test_data.shape[2])\n",
    "\n",
    "# Build the 1D CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 3, activation='relu', input_shape=(train_data.shape[1], train_data.shape[2])))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(train_labels_categorical.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sMaWrvPRxoeR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMaWrvPRxoeR",
    "outputId": "7b959fa7-8a07-4d1e-af5b-77b2b8f02729"
   },
   "outputs": [],
   "source": [
    "np.shape(train_data_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VJBZrk8Uxz9E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJBZrk8Uxz9E",
    "outputId": "fc34b6d7-3123-480c-f451-149de5f5bdb6"
   },
   "outputs": [],
   "source": [
    "np.shape(train_labels_categorical,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gd0UFYHdxgej",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "gd0UFYHdxgej",
    "outputId": "aba6bfc6-dc29-4bd3-a611-2567b4628b92"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels_categorical)\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4U6C5B8XvoRI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "4U6C5B8XvoRI",
    "outputId": "d4b55bd1-1d01-4935-f167-8a7ffd1542cf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming your input data is stored in variables x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "# Reshape the data to have the expected shape\n",
    "x_train = np.reshape(train_data, (train_data.shape[0], 129, 36))\n",
    "x_val = np.reshape(val_data, (val_data.shape[0], 129, 36))\n",
    "x_test = np.reshape(test_data, (test_data.shape[0], 129, 36))\n",
    "\n",
    "# Create the sequential model and add layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(129, 36)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kDybJ4HEmche",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "id": "kDybJ4HEmche",
    "outputId": "0bb512fa-4adb-4a49-af73-2ab8684232f3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(129, 9)),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "history = model.fit(train_data, train_labels, epochs=10, validation_data=(val_data, val_labels))\n",
    "\n",
    "# evaluate the model on test set\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kBklZ3Daq5mI",
   "metadata": {
    "id": "kBklZ3Daq5mI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8Go8MWqxMUl7",
   "metadata": {
    "id": "8Go8MWqxMUl7"
   },
   "outputs": [],
   "source": [
    "indices = np.random.permutation(dataall.shape[0])\n",
    "\n",
    "# Use the shuffled indices to shuffle both data and labels\n",
    "shuffled_data = dataall[indices]\n",
    "shuffled_labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VIHUDFPeRf_f",
   "metadata": {
    "id": "VIHUDFPeRf_f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0c61b-a5e6-496b-ba7c-b6447bc7b47f",
   "metadata": {
    "id": "4fb0c61b-a5e6-496b-ba7c-b6447bc7b47f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data_equally(X, y, test_size=0.4, random_state=None):\n",
    "    # Get unique labels in y and their indices\n",
    "    labels, indices = np.unique(y, return_index=True)\n",
    "    n_labels = len(labels)\n",
    "    \n",
    "    # Split each label's indices into test and train\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    for i in range(n_labels):\n",
    "        label_indices = np.where(y == labels[i])[0]\n",
    "        label_train_indices, label_test_indices = train_test_split(label_indices, test_size=test_size, \n",
    "                                                                    random_state=random_state)\n",
    "        train_indices += list(label_train_indices)\n",
    "        test_indices += list(label_test_indices)\n",
    "    \n",
    "    # Shuffle train and test indices\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "    # Get corresponding data and labels\n",
    "    X_train, y_train = X[train_indices], y[train_indices]\n",
    "    X_test, y_test = X[test_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea210173-b18b-4786-9551-6db9823f80a6",
   "metadata": {
    "id": "ea210173-b18b-4786-9551-6db9823f80a6"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data_equally(shuffled_data,shuffled_labels,test_size=0.3 )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
