{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a72220e-6be1-43bd-bc16-a8c0a8733d4a",
   "metadata": {
    "id": "2a72220e-6be1-43bd-bc16-a8c0a8733d4a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.fft import fft\n",
    "import random\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "qvblyTa8g-X7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qvblyTa8g-X7",
    "outputId": "24f1c0d1-46ff-4e37-ad4f-84bf04b47699"
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.io import loadmat\n",
    "faultI10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_1_10hz.mat')\n",
    "faultI30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_1_30hz.mat')\n",
    "faultII10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_2_10hz.mat')\n",
    "faultII30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_2_30hz.mat')\n",
    "faultIII10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_3_10hz.mat')\n",
    "faultIII30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_3_30hz.mat')\n",
    "faultIV10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_4_10hz.mat')\n",
    "faultIV30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_4_30hz.mat')\n",
    "faultV10=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_5_10hz.mat')\n",
    "faultV30=scipy.io.loadmat('/home/sepehr/Desktop/Rotating Machine - Data/ZData_Fault_Rotating_Machine/unbalanced fault/without alphabet/Unbalanced.mat/Acquisition_un_5_30hz.mat')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0m2Uist6LyMj",
   "metadata": {
    "id": "0m2Uist6LyMj"
   },
   "outputs": [],
   "source": [
    "sigII30=faultII30['Acquisition_un_2_30hz']\n",
    "sigII10=faultII10['Acquisition_un_2_10hz']\n",
    "del faultII30,faultII10\n",
    "sigI30=faultI30['Acquisition_un_1_30hz']\n",
    "sigI10=faultI10['Acquisition_un_1_10hz']\n",
    "del faultI30,faultI10\n",
    "sigIII10=faultIII10['Acquisition_un_3_10hz']\n",
    "sigIII30=faultIII30['Acquisition_un_3_30hz']\n",
    "del faultIII10,faultIII30\n",
    "sigIV10=faultIV10['Acquisition_un_4_10hz']\n",
    "sigIV30=faultIV30['Acquisition_un_4_30hz']\n",
    "del faultIV10,faultIV30\n",
    "sigV10=faultV10['Acquisition_un_5_10hz']\n",
    "sigV30=faultV30['Acquisition_un_5_30hz']\n",
    "del faultV10,faultV30\n",
    "# sigVI10=faultVI10['Acquisition_un_6_10hz']\n",
    "# sigVI30=faultVI30['Acquisition_un_6_30hz']\n",
    "#  del faultVI10,faultVI30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8BhBzYin7SFO",
   "metadata": {
    "id": "8BhBzYin7SFO"
   },
   "outputs": [],
   "source": [
    "\n",
    "def sliding_window(data, window_size, stride):\n",
    "    \"\"\"\n",
    "    Applies a sliding window to the given data with the specified window size\n",
    "    and stride length. Padding is added at the beginning and end of the data to\n",
    "    ensure all samples are included in windows.\n",
    "    \"\"\"\n",
    "    num_padding = window_size - stride\n",
    "    data_padded = np.pad(data, ((num_padding, num_padding), (0, 0)), 'constant')\n",
    "    window_data = []\n",
    "    for i in range(0, len(data_padded) - window_size + 1, stride):\n",
    "        window = data_padded[i:i+window_size, :]\n",
    "        window_data.append(window)\n",
    "    window_data = np.array(window_data)\n",
    "    window_data = np.transpose(window_data, (0, 2, 1))\n",
    "    return window_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cWxOnPFS7URd",
   "metadata": {
    "id": "cWxOnPFS7URd"
   },
   "outputs": [],
   "source": [
    "windowsize=1500\n",
    "stride=1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "zAVETMOk7XKF",
   "metadata": {
    "id": "zAVETMOk7XKF"
   },
   "outputs": [],
   "source": [
    "sigtI10hz=sliding_window(sigI10,windowsize,stride)[:,1:13,:]\n",
    "sigtI30hz=sliding_window(sigI30,windowsize,stride)[:,1:13,:]\n",
    "sigtII10hz=sliding_window(sigII10,windowsize,stride)[:,1:13,:]\n",
    "sigtII30hz=sliding_window(sigII30,windowsize,stride)[:,1:13,:]\n",
    "del sigI10,sigI30,sigII10,sigII30\n",
    "sigtIII30hz=sliding_window(sigIII30,windowsize,stride)[:,1:13,:]\n",
    "sigtIII10hz=sliding_window(sigIII10,windowsize,stride)[:,1:13,:]\n",
    "sigtIV30hz=sliding_window(sigIV30,windowsize,stride)[:,1:13,:]\n",
    "sigtIV10hz=sliding_window(sigIV10,windowsize,stride)[:,1:13,:]\n",
    "del sigIII30,sigIII10,sigIV30,sigIV10\n",
    "sigtV30hz=sliding_window(sigV30,windowsize,stride)[:,1:13,:]\n",
    "sigtV10hz=sliding_window(sigV10,windowsize,stride)[:,1:13,:]\n",
    "# sigtVI30hz=sliding_window(sigVI30,windowsize,stride)\n",
    "# sigtVI10hz=sliding_window(sigVI10,windowsize,stride)\n",
    "# del sigV30,sigV10,sigVI30,sigVI10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eztt9Ct5MMrb",
   "metadata": {
    "id": "eztt9Ct5MMrb"
   },
   "outputs": [],
   "source": [
    "datalable1=np.concatenate([sigtI10hz,sigtI30hz],axis=-1)\n",
    "\n",
    "datalable2=np.concatenate([sigtII10hz,sigtII30hz],axis=-1)\n",
    "\n",
    "datalable3=np.concatenate([sigtIII10hz,sigtIII30hz],axis=-1)\n",
    "\n",
    "datalable4=np.concatenate([sigtIV10hz,sigtIV30hz],axis=-1)\n",
    "\n",
    "datalabele5=np.concatenate([sigtV10hz,sigtV30hz],axis=-1)\n",
    "\n",
    "# datalabele6=np.concatenate([sigtBI10,sigtBI30],axis=-1)\n",
    "\n",
    "# datalabele7=np.concatenate([sigtBII10,sigtBII30],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "XfUB8VSqMT97",
   "metadata": {
    "id": "XfUB8VSqMT97"
   },
   "outputs": [],
   "source": [
    "labels=np.concatenate((np.ones(np.shape(datalable1)[0]),np.zeros(np.shape(datalable1)[0]),np.ones(np.shape(datalable1)[0])*4,np.ones(np.shape(datalable1)[0])*2,np.ones(np.shape(datalable1)[0])*3),axis=0)\n",
    "dataall=np.concatenate([datalable1,datalable2,datalable3,datalable4,datalabele5],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "l0TCUypy5LqU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0TCUypy5LqU",
    "outputId": "d3660a19-98de-4b1f-870e-4a0e6f9ac0d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2150,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "k9RPIXUL0ab-",
   "metadata": {
    "id": "k9RPIXUL0ab-"
   },
   "outputs": [],
   "source": [
    "indices = np.random.permutation(dataall.shape[0])\n",
    "\n",
    "# Use the shuffled indices to shuffle both data and labels\n",
    "shuffled_data = dataall[indices]\n",
    "shuffled_labels = labels[indices]\n",
    "dataall= shuffled_data\n",
    "labels=shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "PEiUiakUTFoV",
   "metadata": {
    "id": "PEiUiakUTFoV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2150, 12, 3000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(dataall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "IFFKaa2-tS8U",
   "metadata": {
    "id": "IFFKaa2-tS8U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 14:24:19.407462: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-05 14:24:19.636699: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-05 14:24:19.638152: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-05 14:24:21.623798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import DepthwiseConv2D, Flatten, Dense, Dropout\n",
    "\n",
    "# reshape the data to have 4 dimensions\n",
    "\n",
    "data = dataall.reshape(dataall.shape[0], dataall.shape[1], dataall.shape[2], 1)\n",
    "\n",
    "# create labels\n",
    "\n",
    "\n",
    "# split the data into training and testing sets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "z7rsrhWPSusl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z7rsrhWPSusl",
    "outputId": "190430fa-6ca9-4c91-e983-74ced0ffd5e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2150, 12, 3000, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "YRCI89bASpG9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "YRCI89bASpG9",
    "outputId": "19757d3f-0272-407d-8f42-2230d7d044b5"
   },
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2b2cb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 14:24:25.373211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-05 14:24:25.374270: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 14:24:25.877195: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 198144000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 5) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n\u001b[1;32m     39\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 40\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_data, train_labels, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filevf34blel.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/sepehr/Desktop/Rotating-Machine-Deep-Learning/.venv/lib/python3.11/site-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 5) are incompatible\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "input_shape = (12, 3000)\n",
    "num_classes = 5\n",
    "\n",
    "# Define input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# First scale\n",
    "conv1 = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "\n",
    "# Second scale\n",
    "conv2 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(pool1)\n",
    "pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "\n",
    "# Third scale\n",
    "conv3 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(pool2)\n",
    "pool3 = MaxPooling1D(pool_size=2)(conv3)\n",
    "\n",
    "# Four scale\n",
    "# conv4 = Conv1D(filters=256, kernel_size=1, activation='relu', padding='same')(pool3)\n",
    "# pool4 = MaxPooling1D(pool_size=1)(conv4)\n",
    "\n",
    "# Flatten and concatenate scales\n",
    "flatten = Flatten()(conv3)\n",
    "dense1 = Dense(128, activation='relu')(flatten)\n",
    "dropout1 = Dropout(0.7)(dense1)\n",
    "\n",
    "# Add final output layer\n",
    "outputs = Dense(num_classes, activation='softmax')(dropout1)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_data, train_labels, validation_split=0.2, epochs=150, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set with multi-class labels\n",
    "test_loss_multi, test_acc_multi = model.evaluate(x_test, y_test1, verbose=2)\n",
    "print('Test accuracy multi:', test_acc_multi)\n",
    "\n",
    "# Convert y_test1 to a multi-class format\n",
    "y_pred_multi = model.predict(x_test)\n",
    "y_test_pred_multi = np.argmax(y_pred_multi, axis=1)\n",
    "\n",
    "# Compute the confusion matrix for multi-class\n",
    "print('Multi-class:', confusion_matrix(np.argmax(y_test1, axis=1), y_test_pred_multi))\n",
    "\n",
    "# Evaluate the model on the test set with normal labels\n",
    "test_loss_normal, test_acc_normal = model.evaluate(x_test, y_test1, verbose=2)\n",
    "print('Test accuracy normal:', test_acc_normal)\n",
    "\n",
    "# Convert y_test to a multi-class format\n",
    "y_pred_normal = model.predict(x_test)\n",
    "y_test_pred_normal = np.argmax(y_pred_normal, axis=1)\n",
    "\n",
    "# Compute the confusion matrix for normal labels\n",
    "print('Normal labels:', confusion_matrix(y_test, y_test_pred_normal))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
